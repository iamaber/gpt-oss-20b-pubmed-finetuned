{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac14faac-1145-4f63-8cd0-08040517c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cloud gpu runpod.io\n",
    "\n",
    "# !pip install torch==2.1.0+cu121 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35b9cb0f-6140-4ad3-9679-0df616131a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d383939233a94d6898ca5a6b4f7702ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98dd35cdb52644a6932c28ed5cef94c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b971f4449b3143a3ab9fcdfb36d3f7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f543d3ec3f484a1c85369f6877b04910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Load PubMedQA dataset\n",
    "dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_labeled\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(example):\n",
    "    # Combine context and question\n",
    "    context = \" \".join(example[\"context\"][\"contexts\"])\n",
    "    input_text = f\"Context: {context}\\n\\nQuestion: {example['question']}\\n\\nAnswer:\"\n",
    "    \n",
    "    # Format labels\n",
    "    label = \"yes\" if example[\"final_decision\"] == \"yes\" else \"no\"\n",
    "    \n",
    "    return {\n",
    "        \"input_text\": input_text,\n",
    "        \"label\": label\n",
    "    }\n",
    "\n",
    "# Apply preprocessing\n",
    "processed_dataset = dataset.map(\n",
    "    preprocess,\n",
    "    remove_columns=[\"pubid\", \"question\", \"context\", \"long_answer\", \"final_decision\"]\n",
    ")\n",
    "\n",
    "# Split dataset\n",
    "train_test = processed_dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "train_dataset = train_test[\"train\"]\n",
    "eval_dataset = train_test[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c6587-1d4d-4604-a28e-a01c638aa4eb",
   "metadata": {},
   "source": [
    "### Load & tokenise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c89ebc-4b78-43f6-a3d7-5ba67c463f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Quantization config (4-bit)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"openai/gpt-oss-20b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Set pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8480e3-b907-4874-9ce9-beffe68db113",
   "metadata": {},
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f48a6b-d3ce-4507-861c-ece7564b8495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # Should show ~0.01% trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf1096e-6eda-4350-84ac-52f671fa8224",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8b5193-892f-4fb9-8876-54cca61fc79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_format(example):\n",
    "    # Tokenize inputs\n",
    "    tokenized = tokenizer(\n",
    "        example[\"input_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    \n",
    "    # Tokenize labels\n",
    "    labels = tokenizer(\n",
    "        example[\"label\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=8,\n",
    "    )[\"input_ids\"]\n",
    "    \n",
    "    # Combine input and label tokens\n",
    "    tokenized[\"input_ids\"] = tokenized[\"input_ids\"] + labels\n",
    "    tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"] + [1] * len(labels)\n",
    "    \n",
    "    # Create labels for loss calculation\n",
    "    tokenized[\"labels\"] = [-100] * len(tokenized[\"input_ids\"]) + labels\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = train_dataset.map(tokenize_and_format)\n",
    "eval_dataset = eval_dataset.map(tokenize_and_format)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(\"torch\")\n",
    "eval_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee613b27-2c72-439c-bc9d-7499a22a0634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt-pubmedqa-20b\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"gpt-20b-pubmedqa\",\n",
    "    gradient_checkpointing=True,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da88605e-c739-4cc9-9097-0e5e88381666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=lambda data: {\n",
    "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in data]),\n",
    "        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in data]),\n",
    "        \"labels\": torch.stack([item[\"labels\"] for item in data]),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "trainer.save_model(\"./gpt-pubmedqa-20b-final\")\n",
    "tokenizer.save_pretrained(\"./gpt-pubmedqa-20b-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d30cc48-f38d-4302-a77e-505aa9e5e051",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d052742b-9b69-4b9d-bfa4-3156f3c18a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"openai/gpt-oss-20b\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"./gpt-pubmedqa-20b-final\")\n",
    "\n",
    "# Inference function\n",
    "def ask_medical_question(context, question):\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# Example usage\n",
    "context = \"Patient shows symptoms X, Y, and Z.\"\n",
    "question = \"Should we administer treatment A?\"\n",
    "print(ask_medical_question(context, question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
