{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nprint(f\"GPU name: {torch.cuda.get_device_name(0)}\")\nprint(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T05:25:59.798596Z","iopub.execute_input":"2025-08-29T05:25:59.798875Z","iopub.status.idle":"2025-08-29T05:26:04.061429Z","shell.execute_reply.started":"2025-08-29T05:25:59.798843Z","shell.execute_reply":"2025-08-29T05:26:04.060807Z"}},"outputs":[{"name":"stdout","text":"GPU available: True\nGPU name: Tesla P100-PCIE-16GB\nGPU memory: 17.059545088 GB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Load and Prepare PubMedQA Dataset","metadata":{}},{"cell_type":"code","source":"\"\"\"The PubMedQA dataset contains biomedical research questions with yes/no/maybe answers,\ncollected from PubMed abstracts 310. The dataset includes questions, contexts (abstracts), and answer labels.\"\"\"\nfrom datasets import load_dataset\n\n# Load PubMedQA dataset\ndataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_labeled\")\n\n# Explore dataset structure\nprint(dataset)\nprint(\"\\nFirst sample:\")\nprint(dataset['train'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T05:30:30.629602Z","iopub.execute_input":"2025-08-29T05:30:30.630063Z","iopub.status.idle":"2025-08-29T05:30:31.302960Z","shell.execute_reply.started":"2025-08-29T05:30:30.630033Z","shell.execute_reply":"2025-08-29T05:30:31.302370Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['pubid', 'question', 'context', 'long_answer', 'final_decision'],\n        num_rows: 1000\n    })\n})\n\nFirst sample:\n{'pubid': 21645374, 'question': 'Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?', 'context': {'contexts': ['Programmed cell death (PCD) is the regulated death of cells within an organism. The lace plant (Aponogeton madagascariensis) produces perforations in its leaves through PCD. The leaves of the plant consist of a latticework of longitudinal and transverse veins enclosing areoles. PCD occurs in the cells at the center of these areoles and progresses outwards, stopping approximately five cells from the vasculature. The role of mitochondria during PCD has been recognized in animals; however, it has been less studied during PCD in plants.', 'The following paper elucidates the role of mitochondrial dynamics during developmentally regulated PCD in vivo in A. madagascariensis. A single areole within a window stage leaf (PCD is occurring) was divided into three areas based on the progression of PCD; cells that will not undergo PCD (NPCD), cells in early stages of PCD (EPCD), and cells in late stages of PCD (LPCD). Window stage leaves were stained with the mitochondrial dye MitoTracker Red CMXRos and examined. Mitochondrial dynamics were delineated into four categories (M1-M4) based on characteristics including distribution, motility, and membrane potential (ΔΨm). A TUNEL assay showed fragmented nDNA in a gradient over these mitochondrial stages. Chloroplasts and transvacuolar strands were also examined using live cell imaging. The possible importance of mitochondrial permeability transition pore (PTP) formation during PCD was indirectly examined via in vivo cyclosporine A (CsA) treatment. This treatment resulted in lace plant leaves with a significantly lower number of perforations compared to controls, and that displayed mitochondrial dynamics similar to that of non-PCD cells.'], 'labels': ['BACKGROUND', 'RESULTS'], 'meshes': ['Alismataceae', 'Apoptosis', 'Cell Differentiation', 'Mitochondria', 'Plant Leaves'], 'reasoning_required_pred': ['y', 'e', 's'], 'reasoning_free_pred': ['y', 'e', 's']}, 'long_answer': 'Results depicted mitochondrial dynamics in vivo as PCD progresses within the lace plant, and highlight the correlation of this organelle with other organelles during developmental PCD. To the best of our knowledge, this is the first report of mitochondria and chloroplasts moving on transvacuolar strands to form a ring structure surrounding the nucleus during developmental PCD. Also, for the first time, we have shown the feasibility for the use of CsA in a whole plant system. Overall, our findings implicate the mitochondria as playing a critical and early role in developmentally regulated PCD in the lace plant.', 'final_decision': 'yes'}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Convert PubMedQA format to instruction format compatible with gpt-oss\n\n\"\"\"This function formats the PubMedQA samples into the Harmony chat format required by gpt-oss models,\nwhich uses a system message, user message, and assistant response structure\"\"\"\n\ndef format_pubmedqa_sample(sample):\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful medical AI assistant. Answer the question based on the provided context.\"},\n            {\"role\": \"user\", \"content\": f\"Context: {sample['context']}\\n\\nQuestion: {sample['question']}\\n\\nAnswer (yes/no/maybe):\"},\n            {\"role\": \"assistant\", \"content\": sample['final_decision']}\n        ]\n    }\n\n# Apply formatting\nformatted_dataset = dataset.map(format_pubmedqa_sample, remove_columns=dataset['train'].column_names)\n\n# Split dataset into train and validation\ntrain_dataset = formatted_dataset['train'].train_test_split(test_size=0.1, seed=42)\ntrain_data = train_dataset['train']\nval_data = train_dataset['test']\n\nprint(train_data[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T05:32:11.367109Z","iopub.execute_input":"2025-08-29T05:32:11.367664Z","iopub.status.idle":"2025-08-29T05:32:11.380258Z","shell.execute_reply.started":"2025-08-29T05:32:11.367626Z","shell.execute_reply":"2025-08-29T05:32:11.379512Z"}},"outputs":[{"name":"stdout","text":"{'messages': [{'content': 'You are a helpful medical AI assistant. Answer the question based on the provided context.', 'role': 'system'}, {'content': \"Context: {'contexts': ['In recent clinical trials (RCT) of bowel preparation, Golytely was more efficacious than MiraLAX. We hypothesised that there is a difference in adenoma detection between Golytely and MiraLAX.', 'To compare the adenoma detection rate (ADR) between these bowel preparations, and to identify independent predictors of bowel preparation quality and adenoma detection.', 'This was a post hoc analysis of an RCT that assessed efficacy and patient tolerability of Golytely vs. MiraLAX/Gatorade in average risk screening colonoscopy patients. Bowel preparation quality was measured with the Boston Bowel Preparation Scale (BBPS). An excellent/good equivalent BBPS score was defined as ≥ 7. Polyp pathology review was performed. ADR was defined as the proportion of colonoscopies with an adenoma. Univariate and multivariate analyses were conducted.', 'One hundred and ninety patients were prospectively enrolled (87 MiraLAX, 103 Golytely). Golytely had a higher rate of a BBPS score ≥ 7 (82.5% vs. MiraLAX 67.8%, P=0.02). The ADR in the Golytely cohort was 26.2% (27/103), and was 16.1% (14/87) for MiraLAX (P = 0.091). On multivariate analyses, Golytely was 2.13 × more likely to be associated with a BBPS ≥ 7 (95% CI 1.05-4.32, P = 0.04) and 2.28 × more likely to be associated with adenoma detection (95% CI 1.05-4.98, P = 0.04) than MiraLAX.'], 'labels': ['BACKGROUND', 'AIMS', 'METHODS', 'RESULTS'], 'meshes': ['Adenoma', 'Age Factors', 'Aged', 'Colonoscopy', 'Colorectal Neoplasms', 'Electrolytes', 'Female', 'Humans', 'Male', 'Mass Screening', 'Middle Aged', 'Multivariate Analysis', 'Polyethylene Glycols', 'Preoperative Care', 'Randomized Controlled Trials as Topic', 'Retrospective Studies', 'Solvents'], 'reasoning_required_pred': ['y', 'e', 's'], 'reasoning_free_pred': ['y', 'e', 's']}\\n\\nQuestion: MiraLAX vs. Golytely: is there a significant difference in the adenoma detection rate?\\n\\nAnswer (yes/no/maybe):\", 'role': 'user'}, {'content': 'yes', 'role': 'assistant'}]}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### Load Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"\"\"\"We use 4-bit quantization to reduce memory usage, allowing the 20B parameter model to fit on the A40 GPU.\nThe BitsAndBytesConfig configures the quantization parameters\"\"\"\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model\nimport torch\n\n# Model configuration\nmodel_name = \"openai/gpt-oss-20b\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Load model with quantization\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n    torch_dtype=torch.bfloat16,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T05:55:48.860538Z","iopub.execute_input":"2025-08-29T05:55:48.861367Z","iopub.status.idle":"2025-08-29T05:55:48.895544Z","shell.execute_reply.started":"2025-08-29T05:55:48.861333Z","shell.execute_reply":"2025-08-29T05:55:48.894702Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.models.auto.modeling_auto'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/921013086.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBitsAndBytesConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"We use 4-bit quantization to reduce memory usage, allowing the 20B parameter model to fit on the A40 GPU.\n\u001b[1;32m      5\u001b[0m The BitsAndBytesConfig configures the quantization parameters\"\"\"\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: Could not import module 'AutoModelForCausalLM'. Are this object's requirements defined correctly?"],"ename":"ModuleNotFoundError","evalue":"Could not import module 'AutoModelForCausalLM'. Are this object's requirements defined correctly?","output_type":"error"}],"execution_count":26},{"cell_type":"markdown","source":"### Configure LoRA for Parameter-Efficient Fine-Tuning","metadata":{}},{"cell_type":"code","source":"# LoRA configuration\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\n        \"7.mlp.experts.gate_up_proj\",\n        \"7.mlp.experts.down_proj\",\n        \"15.mlp.experts.gate_up_proj\",\n        \"15.mlp.experts.down_proj\",\n        \"23.mlp.experts.gate_up_proj\",\n        \"23.mlp.experts.down_proj\",\n    ],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Prepare model for PEFT\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training ","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./gpt-oss-20b-pubmedqa\",\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=3,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    evaluation_strategy=\"epoch\",\n    report_to=\"wandb\",\n    fp16=False,\n    bf16=True,\n    max_grad_norm=0.3,\n    warmup_ratio=0.03,\n    lr_scheduler_type=\"cosine\",\n    remove_unused_columns=False,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import DataCollatorForLanguageModeling\n\n# Create trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n    dataset_text_field=\"messages\",\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n    max_seq_length=2048,\n    packing=False,\n)\n\n# Start training\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the fine-tuned model\ntrainer.save_model()\ntokenizer.save_pretrained(\"./gpt-oss-20b-pubmedqa\")\n\n# Evaluate the model\neval_results = trainer.evaluate()\nprint(f\"Evaluation results: {eval_results}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the fine-tuned model for inference\nfrom peft import PeftModel\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n)\nfine_tuned_model = PeftModel.from_pretrained(base_model, \"./gpt-oss-20b-pubmedqa\")\n\n# Create inference pipeline\npipe = pipeline(\n    \"text-generation\",\n    model=fine_tuned_model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\n\n# Test with a biomedical question\ntest_question = {\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful medical AI assistant. Answer the question based on the provided context.\"},\n        {\"role\": \"user\", \"content\": \"Context: This study examines the effects of aspirin on cardiovascular risk in diabetic patients.\\n\\nQuestion: Does aspirin reduce cardiovascular risk in diabetes?\\n\\nAnswer (yes/no/maybe):\"}\n    ]\n}\n\nformatted_prompt = tokenizer.apply_chat_template(test_question[\"messages\"], tokenize=False)\noutputs = pipe(formatted_prompt, max_new_tokens=50, temperature=0.1)\nprint(outputs[0]['generated_text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}