{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline,\n)\nfrom peft import LoraConfig, PeftModel, get_peft_model\nfrom trl import SFTTrainer\n\n\nprint(f\"GPU name: {torch.cuda.get_device_name(0)}\")\nprint(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T09:38:31.305096Z","iopub.execute_input":"2025-08-29T09:38:31.305284Z","iopub.status.idle":"2025-08-29T09:38:40.726506Z","shell.execute_reply.started":"2025-08-29T09:38:31.305266Z","shell.execute_reply":"2025-08-29T09:38:40.725737Z"}},"outputs":[{"name":"stderr","text":"2025-08-29 09:38:36.899620: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756460316.922629     153 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756460316.929710     153 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"GPU name: Tesla P100-PCIE-16GB\nGPU memory: 17.059545088 GB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T09:38:40.727774Z","iopub.execute_input":"2025-08-29T09:38:40.728483Z","iopub.status.idle":"2025-08-29T09:38:40.745810Z","shell.execute_reply.started":"2025-08-29T09:38:40.728462Z","shell.execute_reply":"2025-08-29T09:38:40.744922Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d80ff1b8e09479190abd1072b274ad5"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Set seed for reproducibility\nimport random\n\ndef set_seed(seed=11):\n    \"\"\"Set all seeds to ensure reproducibility\"\"\"\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T09:45:04.670018Z","iopub.execute_input":"2025-08-29T09:45:04.670682Z","iopub.status.idle":"2025-08-29T09:45:04.675625Z","shell.execute_reply.started":"2025-08-29T09:45:04.670654Z","shell.execute_reply":"2025-08-29T09:45:04.674924Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Load Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load PubMedQA dataset\npubmedqa = load_dataset(\"qiaojin/PubMedQA\", \"pqa_labeled\")\n\n# Split the dataset\ntrain_test_split = pubmedqa[\"train\"].train_test_split(test_size=0.1)\ndataset = train_test_split\n\n# Examine the dataset\nprint(dataset)\nprint(dataset[\"train\"][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T09:38:40.746798Z","iopub.execute_input":"2025-08-29T09:38:40.747082Z","iopub.status.idle":"2025-08-29T09:38:42.620134Z","shell.execute_reply.started":"2025-08-29T09:38:40.747048Z","shell.execute_reply":"2025-08-29T09:38:42.619502Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['pubid', 'question', 'context', 'long_answer', 'final_decision'],\n        num_rows: 900\n    })\n    test: Dataset({\n        features: ['pubid', 'question', 'context', 'long_answer', 'final_decision'],\n        num_rows: 100\n    })\n})\n{'pubid': 20101129, 'question': 'Is prophylactic fixation a cost-effective method to prevent a future contralateral fragility hip fracture?', 'context': {'contexts': [': A previous hip fracture more than doubles the risk of a contralateral hip fracture. Pharmacologic and environmental interventions to prevent hip fracture have documented poor compliance. The purpose of this study was to examine the cost-effectiveness of prophylactic fixation of the uninjured hip to prevent contralateral hip fracture.', ': A Markov state-transition model was used to evaluate the cost and quality-adjusted life-years (QALYs) for unilateral fixation of hip fracture alone (including internal fixation or arthroplasty) compared with unilateral fixation and contralateral prophylactic hip fixation performed at the time of hip fracture or unilateral fixation and bilateral hip pad protection. Prophylactic fixation involved placement of a cephalomedullary nail in the uninjured hip and was initially assumed to have a relative risk of a contralateral fracture of 1%. Health states included good health, surgery-related complications requiring a second operation (infection, osteonecrosis, nonunion, and malunion), fracture of the uninjured hip, and death. The primary outcome measure was the incremental cost-effectiveness ratio estimated as cost per QALY gained in 2006 US dollars with incremental cost-effectiveness ratios below $50,000 per QALY gained considered cost-effective. Sensitivity analyses evaluated the impact of patient age, annual mortality and complication rates, intervention effectiveness, utilities, and costs on the value of prophylactic fixation.', ': In the baseline analysis, in a 79-year-old woman, prophylactic fixation was not found to be cost-effective (incremental cost-effectiveness ratio = $142,795/QALY). However, prophylactic fixation was found to be a cost-effective method to prevent contralateral hip fracture in: 1) women 71 to 75 years old who had 30% greater relative risk for a contralateral fracture; and 2) women younger than age 70 years. Cost-effectiveness was greater when the additional costs of prophylaxis were less than $6000. However, for most analyses, the success of prophylactic fixation was highly sensitive to the effectiveness and the relative morbidity and mortality of the additional procedure.'], 'labels': ['OBJECTIVE', 'METHODS', 'RESULTS'], 'meshes': ['Age Factors', 'Aged', 'Aged, 80 and over', 'Bone Nails', 'Cost-Benefit Analysis', 'Female', 'Fracture Fixation, Internal', 'Hip Fractures', 'Humans', 'Male', 'Markov Chains', 'Middle Aged', 'Primary Prevention', 'Protective Clothing', 'Quality-Adjusted Life Years', 'Sex Factors'], 'reasoning_required_pred': ['m', 'a', 'y', 'b', 'e'], 'reasoning_free_pred': ['m', 'a', 'y', 'b', 'e']}, 'long_answer': ': Prophylactic fixation with a cephalomedullary nail was not found to be cost-effective for the average older woman who sustained a hip fracture. However, it may be appropriate for select patient populations. The study supports the need for basic science and clinical trials investigating the effectiveness of prophylactic fixation for patient populations at higher lifetime risk for contralateral hip fracture.', 'final_decision': 'maybe'}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Preprocess the Data","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load tokenizer for Mistral-7B-Instruct\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token  # Set pad token to EOS token\n\ndef format_prompt(example):\n    # Format the prompt using Mistral's instruction format\n    prompt = f\"<s>[INST] Context: {example['context']}\\n\\nQuestion: {example['question']} [/INST] {example['final_decision']}</s>\"\n    return {\"text\": prompt}\n\ndef tokenize_function(examples):\n    # Tokenize the formatted prompts\n    return tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n\n# Apply formatting and tokenization\ndataset = dataset.map(format_prompt)\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T09:38:42.621443Z","iopub.execute_input":"2025-08-29T09:38:42.621654Z","iopub.status.idle":"2025-08-29T09:38:44.925087Z","shell.execute_reply.started":"2025-08-29T09:38:42.621637Z","shell.execute_reply":"2025-08-29T09:38:44.924172Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba328999a95e4a3e940ce64a3f1252f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d68a8de95984471b0231566383c14e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ac5b4191f554ca1bd9ad5af13285e2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2341ead13fa4199bfdbf0cb084b72d6"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"### Fine-tuning\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\n# Configure 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=False,\n)\n\n# Load the model with 4-bit quantization\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\n# Prepare the model for training\nmodel = prepare_model_for_kbit_training(model)\n\n# Configure LoRA (Low-Rank Adaptation)\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T09:38:44.926749Z","iopub.execute_input":"2025-08-29T09:38:44.927136Z","iopub.status.idle":"2025-08-29T09:41:48.745557Z","shell.execute_reply.started":"2025-08-29T09:38:44.927105Z","shell.execute_reply":"2025-08-29T09:41:48.744839Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68e5a6c64ec44be5a2e8ea405cd3861f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2921947954a0497cb7ebbd8cb42fea86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1317ea9cc4de4183bced1e4c5750cace"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33724f88917e425c861276fe3c043e57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad3c42fc25ee43c3a0dcaccd72e4b43d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afed608805f744d19b05de4fe67bb805"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3acae94837074a34aa26fefc1d43484f"}},"metadata":{}},{"name":"stdout","text":"trainable params: 41,943,040 || all params: 7,289,966,592 || trainable%: 0.5754\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Train the Model","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./mistral-pubmedqa\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    num_train_epochs=3,\n    logging_steps=10,\n    save_steps=500,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    fp16=True,  # Use mixed precision\n    optim=\"paged_adamw_8bit\",\n    report_to=\"wandb\",\n    run_name=\"mistral-pubmedqa-finetune\",\n    gradient_checkpointing=True,\n    ddp_find_unused_parameters=False,\n    remove_unused_columns=False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T09:43:21.239513Z","iopub.execute_input":"2025-08-29T09:43:21.239800Z","iopub.status.idle":"2025-08-29T09:43:21.266907Z","shell.execute_reply.started":"2025-08-29T09:43:21.239780Z","shell.execute_reply":"2025-08-29T09:43:21.265809Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_153/3111635659.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Define training arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./mistral-pubmedqa\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mper_device_train_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"],"ename":"TypeError","evalue":"TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    data_collator=lambda data: {\n        \"input_ids\": torch.stack([item[\"input_ids\"] for item in data]),\n        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in data]),\n        \"labels\": torch.stack([item[\"input_ids\"] for item in data]),\n    },\n)\n\n# Start training\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Save the model","metadata":{}},{"cell_type":"code","source":"# Save the model\ntrainer.save_model(\"./mistral-pubmedqa-finetune\")\n\n# Save the tokenizer\ntokenizer.save_pretrained(\"./mistral-pubmedqa-finetune\")\n\n# Save the LoRA adapter separately\nmodel.save_pretrained(\"./mistral-pubmedqa-adapter\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluation","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef evaluate_pubmedqa(model, tokenizer, dataset):\n    \"\"\"Evaluate the model on PubMedQA test set\"\"\"\n    predictions = []\n    references = []\n    \n    # Set model to evaluation mode\n    model.eval()\n    \n    # Process each example in the test set\n    for example in dataset:\n        # Format the prompt\n        prompt = f\"<s>[INST] Context: {example['context']}\\n\\nQuestion: {example['question']} [/INST]\"\n        \n        # Tokenize the prompt\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n        \n        # Generate answer\n        with torch.no_grad():\n            outputs = model.generate(\n                input_ids=inputs[\"input_ids\"],\n                attention_mask=inputs[\"attention_mask\"],\n                max_new_tokens=10,\n                do_sample=False,\n                pad_token_id=tokenizer.eos_token_id\n            )\n        \n        # Decode the generated text\n        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # Extract the answer (yes/no/maybe)\n        # The answer should be after the [/INST] token\n        answer = generated_text.split(\"[/INST]\")[-1].strip().lower()\n        \n        # Normalize the answer to match the expected format\n        if \"yes\" in answer:\n            pred = \"yes\"\n        elif \"no\" in answer:\n            pred = \"no\"\n        elif \"maybe\" in answer:\n            pred = \"maybe\"\n        else:\n            # If unclear, default to maybe\n            pred = \"maybe\"\n        \n        predictions.append(pred)\n        references.append(example[\"final_decision\"].lower())\n    \n    # Calculate metrics\n    accuracy = accuracy_score(references, predictions)\n    f1 = f1_score(references, predictions, average=\"weighted\")\n    \n    # Create confusion matrix\n    cm = confusion_matrix(references, predictions, labels=[\"yes\", \"no\", \"maybe\"])\n    \n    # Plot confusion matrix\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"yes\", \"no\", \"maybe\"], yticklabels=[\"yes\", \"no\", \"maybe\"])\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix - PubMedQA Evaluation')\n    plt.savefig('./pubmedqa_confusion_matrix.png')\n    \n    # Print metrics\n    print(f\"PubMedQA Evaluation Results:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    \n    return {\n        \"accuracy\": accuracy,\n        \"f1\": f1,\n        \"confusion_matrix\": cm\n    }\n\n# Evaluate on PubMedQA test set\npubmedqa_results = evaluate_pubmedqa(model, tokenizer, dataset[\"test\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluation on MMLU","metadata":{}},{"cell_type":"code","source":"from lm_eval import evaluator\nfrom lm_eval.models.huggingface import HFLM\n\ndef evaluate_mmlu(model, tokenizer):\n    \"\"\"Evaluate the model on MMLU dataset\"\"\"\n    # Create HFLM model for lm-eval\n    hf_model = HFLM(\n        pretrained=model,\n        tokenizer=tokenizer,\n        batch_size=4,\n        device=\"cuda\"\n    )\n    \n    # Define MMLU tasks\n    # We'll evaluate on a subset of MMLU tasks for demonstration\n    # You can add more tasks as needed\n    tasks = [\n        \"mmlu_anatomy\",\n        \"mmlu_clinical_knowledge\",\n        \"mmlu_college_medicine\",\n        \"mmlu_medical_genetics\",\n        \"mmlu_professional_medicine\",\n        \"mmlu_virology\"\n    ]\n    \n    # Run evaluation\n    results = evaluator.simple_evaluate(\n        model=hf_model,\n        tasks=tasks,\n        num_fewshot=5,\n        limit=50  # Limit to 50 examples per task for faster evaluation\n    )\n    \n    # Print results\n    print(\"\\nMMLU Evaluation Results:\")\n    for task in tasks:\n        if task in results[\"results\"]:\n            print(f\"{task}: {results['results'][task]['acc']:.4f}\")\n    \n    # Calculate average accuracy\n    avg_acc = np.mean([results[\"results\"][task][\"acc\"] for task in tasks if task in results[\"results\"]])\n    print(f\"\\nAverage MMLU Accuracy: {avg_acc:.4f}\")\n    \n    return results\n\n# Evaluate on MMLU\nmmlu_results = evaluate_mmlu(model, tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\n# Save evaluation results\nevaluation_results = {\n    \"pubmedqa\": pubmedqa_results,\n    \"mmlu\": mmlu_results\n}\n\nwith open(\"./evaluation_results.json\", \"w\") as f:\n    json.dump(evaluation_results, f, indent=4)\n\nprint(\"Evaluation results saved to evaluation_results.json\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load the fine-tuned model\nmodel_path = \"./mistral-pubmedqa-finetune\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\n\n# Create a text generation pipeline\nqa_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\n# Example question\ncontext = \"Recent studies have shown that regular physical activity can reduce the risk of cardiovascular disease by up to 35%.\"\nquestion = \"Does exercise reduce heart disease risk?\"\n\n# Format the prompt\nprompt = f\"<s>[INST] Context: {context}\\n\\nQuestion: {question} [/INST]\"\n\n# Generate answer\nresult = qa_pipeline(prompt, max_new_tokens=10, num_return_sequences=1)\nprint(result[0]['generated_text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}